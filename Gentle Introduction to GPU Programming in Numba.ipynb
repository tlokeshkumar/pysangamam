{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# A Gentle Introduction to GPU Programming using Numba\n",
    "\n",
    "> By Lokesh Kumar T\n",
    "\n",
    "> Sept 2018\n",
    "\n",
    "> IIT Madras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Overview of the Talk\n",
    "\n",
    "- CPUs and GPUs\n",
    "- Whats Heterogenous Computing?\n",
    "- Introduction to GPU Programming Terminologies\n",
    "- Introduction to Numba\n",
    "- Lets multiply a vector by 2 in GPU!\n",
    "- Matrix Multiplication in a GPU\n",
    "- How to proceed further?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "CPUs are generally increasingly good in reducing *latency* for single stream of processing.\n",
    "\n",
    "**Fundamental performance quest in single core CPU:**\n",
    "\n",
    "How to devote transistors on a chip to make a single stream of instructions run faster and faster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- **CPU**: work on a variety of different calculations\n",
    "- **GPU**: best at focusing all the computing abilities on a specific task\n",
    "- **CPU**: few cores (up to 24) optimized for sequential serial processing\n",
    "- **GPU**: thousands of smaller and more efficient cores for a massively parallel architecture\n",
    "- **GPU**s provide **superior processing power**, **memory bandwidth** and **efficiency** over their CPU counterparts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "![trend](https://www.karlrupp.net/wp-content/uploads/2018/02/42-years-processor-trend.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "# Why is can't we go faster?\n",
    "\n",
    "* Power Management\n",
    "* Memory Access Rates\n",
    "* Instruction Level Parallelism"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# How to exploit the Sequential Acceleration of CPU and Parallel Acceleration of GPU?\n",
    "\n",
    "![heterogenous](https://preview.ibb.co/g0QqQ9/heterogenous_computing.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Heterogenous Computing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Basic Idea\n",
    "\n",
    "![basic_idea](https://www.davebennett.tech/wp-content/uploads/2017/12/how-gpu-acceleration-works.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### How does GPU manage to accelerate compute intensive tasks?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Uses parallel programming strategy\n",
    "    - Breaks the tasks into several smaller sub tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Many versions of the sub-tasks operating different data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Works on the sub tasks simultaneously (parallely)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### How does CPU approach this same task?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Uses single thread. Single stream of instructions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Tries to accelerate that single stream of instruction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Sequential Processing!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## What are those `tasks`?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Computational tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Matrix Multiplications\n",
    "- Vector Addition\n",
    "- Fast Fourier Transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Signal Processing techniques\n",
    "- Deep Learning Workloads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Breaking a task into sub-tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Crutial to attain maximum performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Depends from task to task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Some tasks are easier and straight-forward than others"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Lets see an example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Vector Addition\n",
    "\n",
    "\n",
    "- Vectors are columns of numbers\n",
    "\n",
    "$$\\vec{a} = \\begin{bmatrix}1 \\\\2\\\\\\vdots\\\\n\\end{bmatrix}_{n \\times 1}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Lets take 2 vectors $\\vec{a}, \\vec{b}$ both in $\\mathbb{R}^n$ (n-dimensional space)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "$\n",
    "\\vec{a} = \\begin{bmatrix}a_1 \\\\a_2\\\\\\vdots\\\\a_n\\end{bmatrix}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "$\n",
    "\\vec{b} = \\begin{bmatrix}b_1 \\\\b_2\\\\\\vdots\\\\b_n\\end{bmatrix}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Whats $\\vec{a} + \\vec{b}$ ? (Vector addition)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Element wise addition\n",
    "\n",
    "$\n",
    "\\vec{a} + \\vec{b} = \\begin{bmatrix}a_1 + b_1\\\\a_2+b_2\\\\a_3+b_3\\\\\\vdots\\\\a_n+b_n\\end{bmatrix}\n",
    "$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "for i in range(n):\n",
    "    c[i] = a[i] + b[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# How to split it into sub tasks?\n",
    "> In other words parallelize it?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Here are the steps (simple algorithms):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Identify independent instructions (operations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Identify their input of these indepedent operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Finalze your fundamental unit that will have several versions running parallely."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Revisiting Visiting Vector Addition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Whats the fundamental operation performed?\n",
    "- Whats the input for this operation to be performed?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Fundamental Operation: **Addition of 2 numbers**\n",
    "\n",
    "Input: One Elements from 2 vectors **`a[i], b[i]`**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Same operation is performed on different data items. (here $a[i], b[i]$)\n",
    "\n",
    "> **SIMD** Processing - **S**ingle **I**nstruction **M**ultiple **D**ata Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Our Approach to program this addition in GPU:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Replicate `addition` on different compute units in GPUs\n",
    "- Give appropriate inputs to these units so that they perform useful operation.\n",
    "- Aggregate the each units output and send it to CPU\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Terminologies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- **Device**: GPU (Device memory: GPU Memory)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- **Host**: CPU (Host Memory: CPU Memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- **Kernel**: The function that runs in GPU\n",
    "    - Whats our kernel in vector addition?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- **Threads**: The computational units in GPUs. Runs a version of the kernel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- **Blocks**: Collections of a set of threads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- **Grid**: Collection of set of blocks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Lets Code Vector Scaling in GPU using Numba!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Sample Introduction to Numba\n",
    "\n",
    "Numba gives you the power to speed up your applications with high performance functions written directly in Python. \n",
    "\n",
    "We will look into a basic program and understand the Numba programming basics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# !conda install -c numba numba\n",
    "import numba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Managed Device 0>\n"
     ]
    }
   ],
   "source": [
    "from numba import cuda\n",
    "print(cuda.gpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Blocks in one grid:\t4096\n",
      "Threads in one Block:\t256\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# SCALING A VECTOR BY 2\n",
    "# Create the data array - usually initialized some other way\n",
    "data = np.ones(256*4096) # 1,041,664\n",
    "\n",
    "threadsperblock = 256\n",
    "\n",
    "# Ceil function\n",
    "blockspergrid = (data.size + (threadsperblock - 1)) // threadsperblock\n",
    "print (\"Blocks in one grid:\\t\" + str(blockspergrid))\n",
    "print (\"Threads in one Block:\\t\" + str(threadsperblock))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Whats this threadsperblock, blockspergrid business?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- For effective parallelization of higher dimensional data structures, loopy data structures:  \n",
    "    - CUDA follows an hierarchy\n",
    "    - threads, blocks, grids we saw remember?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Hierarchy\n",
    "\n",
    "- **Threads**: The computational units in GPUs. Runs a version of the kernel.\n",
    "- **Blocks**: Collections of a set of threads\n",
    "- **Grid**: Collection of set of blocks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### We defined how many blocks and threads are needed.\n",
    "\n",
    "### Now lets define the kernel function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from numba import cuda\n",
    "\n",
    "@cuda.jit\n",
    "def my_kernel(io_array):\n",
    "    pos = cuda.grid(1)\n",
    "    if pos < io_array.size:  # Check array boundaries\n",
    "        io_array[pos] *= 2 # do the computation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![1D_blocks](https://devblogs.nvidia.com/wp-content/uploads/2017/01/cuda_indexing.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Finding the global index of the thread\n",
    "\n",
    "\n",
    "* **numba.cuda.grid(ndim)** - Return the absolute position of the current thread in the entire grid of blocks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Calling the Kernel from the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13.6 ms ± 66.5 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "# Now start the kernel\n",
    "# And time the GPU execution time also\n",
    "my_kernel[blockspergrid, threadsperblock](data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2. 2. 2. ... 2. 2. 2.]\n"
     ]
    }
   ],
   "source": [
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.35 ms ± 278 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "# timing the CPU Operations\n",
    "data_2 = data*2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "@cuda.jit\n",
    "def my_kernel2(io_array):\n",
    "    pos = cuda.grid(1)\n",
    "    if pos < io_array.size:\n",
    "        io_array[pos] *= 2 # do the computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# Host code   \n",
    "data = np.ones(256*4096)\n",
    "threadsperblock = 256\n",
    "blockspergrid = np.ceil(data.shape[0] / threadsperblock).astype('int32')\n",
    "my_kernel2[blockspergrid, threadsperblock](data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2. 2. 2. ... 2. 2. 2.]\n"
     ]
    }
   ],
   "source": [
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Lets do Matrix Multiplication in GPU!\n",
    "\n",
    "> How will you approach this problem????\n",
    "\n",
    "> How will you assign the threads and blocks? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![matrix_mul](https://2.bp.blogspot.com/-7o5dQnaKQOQ/WATzLVr7kTI/AAAAAAAAHTk/cBbuAENwoKgSqoTw_oPgUsZtVe8LMMOMQCLcB/s1600/Matrix%2BMultiplication%2Bin%2BJava.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Remember the guidelines:\n",
    "\n",
    "- Identify independent instructions (operations)\n",
    "- Identify their input of these indepedent operations\n",
    "- Finalize your fundamental unit that will have several versions running parallely.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![matrix](https://nyu-cds.github.io/python-numba/fig/05-matmul.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## What's the dimension of the block here?\n",
    "\n",
    "> Is it 1D as we saw in scalar multiplication?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![image](https://www.pelagos-consulting.com/wp-content/uploads/2017/08/blocks-e1503566461683.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "![image](https://s3.amazonaws.com/i.seelio.com/6f/fd/6ffd44cf043d8c0e80e4652da28bffb6ae1e.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Lets code this in Numba"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Host Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Host code\n",
    "\n",
    "# Initialize the data arrays\n",
    "m = 2**11 # 2048\n",
    "n = 2**11\n",
    "p = 2**11\n",
    "\n",
    "A = np.full((m, n), 1, np.float) # matrix containing all 1's\n",
    "B = np.full((n, p), 1, np.float) # matrix containing all 1's"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Host to device data transfer + Memory allocation in GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Copy the arrays to the device\n",
    "A_global_mem = cuda.to_device(A)\n",
    "B_global_mem = cuda.to_device(B)\n",
    "\n",
    "# Allocate memory on the device for the result\n",
    "C_global_mem = cuda.device_array((m, p))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "@cuda.jit\n",
    "def matmul(A, B, C):\n",
    "    \"\"\"Perform matrix multiplication of C = A * B\n",
    "    \"\"\"\n",
    "    i, j = cuda.grid(2)\n",
    "    if i < C.shape[0] and j < C.shape[1]:\n",
    "        tmp = 0.\n",
    "        for k in range(A.shape[1]):\n",
    "            tmp += A[i, k] * B[k, j]\n",
    "        C[i, j] = tmp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Defining threadsperblock, blockspergrid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "threadsperblock = (32, 32)\n",
    "# Dimension of the matrix we defined is 2048x2048"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "blockspergrid_x = int(np.ceil(A.shape[0] / threadsperblock[0]))\n",
    "blockspergrid_y = int(np.ceil(B.shape[1] / threadsperblock[1]))\n",
    "blockspergrid = (blockspergrid_x, blockspergrid_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Kernel Call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Start the kernel \n",
    "matmul[blockspergrid, threadsperblock](A_global_mem, B_global_mem, C_global_mem)\n",
    "\n",
    "# Copy the result back to the host\n",
    "C = C_global_mem.copy_to_host()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2048. 2048. 2048. ... 2048. 2048. 2048.]\n",
      " [2048. 2048. 2048. ... 2048. 2048. 2048.]\n",
      " [2048. 2048. 2048. ... 2048. 2048. 2048.]\n",
      " ...\n",
      " [2048. 2048. 2048. ... 2048. 2048. 2048.]\n",
      " [2048. 2048. 2048. ... 2048. 2048. 2048.]\n",
      " [2048. 2048. 2048. ... 2048. 2048. 2048.]]\n"
     ]
    }
   ],
   "source": [
    "print(C)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Lets time it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "299 µs ± 134 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit -n 10\n",
    "matmul[blockspergrid, threadsperblock](A_global_mem, B_global_mem, C_global_mem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "463 ms ± 20.4 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit -n 10\n",
    "C = A.dot(B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# New Moore's Law\n",
    "\n",
    "* Computers no longer get faster, just Wider\n",
    "\n",
    "* Rethink your algorithms to be parallel\n",
    "\n",
    "* Data-Parallel Computing is the most scalable solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![gpu-applications](https://slideplayer.com/slide/3419442/12/images/2/GPU+Accelerated+Applications.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "# Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "![gpu](http://archive.rtcmagazine.com/files/images/5985/RTC08-ERTW-Nvidia-FigX_original_large.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Thanks for your patience\n",
    "\n",
    "This presentation and extensive resources can be found in my GitHub - [tlokeshkumar](github.com/tlokeshkumar).\n",
    "\n",
    "Even projects and other codes in GPU Programming, Deep Learning etc are present in my GitHub.... Do check them out if interested!!!\n",
    "\n",
    "Feel free to contact me at lokesh.karpagam@gmail.com"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
